{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cef41a2-8679-499d-a208-fd3af5e5479e",
   "metadata": {},
   "source": [
    "# Demo looking at relationship between residuals and solar wind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f481c8-7759-4aef-85c8-5e0f9d2b785c",
   "metadata": {},
   "source": [
    "## Prep access to packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a56c36-7aea-492a-9469-3583b61ccce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "import pooch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "import zarr\n",
    "# import holoviews as hv\n",
    "# import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from chaosmagpy.plot_utils import nio_colormap\n",
    "\n",
    "from src.env import ICOS_FILE\n",
    "\n",
    "try:\n",
    "    from src.data.proc_env import INT_FILE_PATHS\n",
    "    from src.env import REFRAD, TMPDIR, ICOS_FILE\n",
    "except ImportError:\n",
    "    print(\"Failed to import src...\")\n",
    "    TMPDIR = os.getcwd()\n",
    "    project_root = os.path.dirname(TMPDIR)\n",
    "    INT_FILE_PATHS = {\"A\": os.path.join(TMPDIR, \"SwA_20140501-20190501_proc1.nc\")}\n",
    "    REFRAD = 6371200\n",
    "    ICOS_FILE = os.path.join(project_root, \"data/external/icosphere_data.h5\")\n",
    "    print(\"Using instead for cube download and scratch space:\", TMPDIR)\n",
    "if not os.path.exists(TMPDIR):\n",
    "    print(\"Can't find scratch space:\", TMPDIR)\n",
    "    TMPDIR = os.getcwd()\n",
    "    print(\"Using instead:\", TMPDIR)\n",
    "\n",
    "xr.set_options(\n",
    "    display_expand_attrs=False,\n",
    "    display_expand_data_vars=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5838b-efe4-4abd-9c8e-419f6fee5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_store = os.path.join(TMPDIR, \"datacube_test.zarr\")\n",
    "ds = xr.open_dataset(\n",
    "    zarr_store, engine=\"zarr\",\n",
    "    chunks=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61eb987-4107-4ec6-a78b-fae2088a56ee",
   "metadata": {},
   "source": [
    "## Load information about the grid points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2adce-9edc-48e4-a3aa-0034c14ce97c",
   "metadata": {},
   "source": [
    "The grid coordinates are stored in a separate file. These are locations within a spherical (theta, phi) shell. The `grid_index` matches up with the numbers given in the `gridpoint_geo` and `gridpoint_qdmlt` variables, so can be used to identify the (theta, phi) coordinates for each bin.\n",
    "\n",
    "- For `gridpoint_geo`, (`theta`, `phi`) correspond to (`90-Latitude`, `Longitude%360`)\n",
    "- For `gridpoint_qdmlt`, (`theta`, `phi`) correspond to (`90-QDLat`, `MLT*15`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e5b75-d41e-4170-96bc-ffea21d627d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the coordinates, stored as \"40962\" within the HDF file.\n",
    "gridcoords = pd.read_hdf(ICOS_FILE, key=\"40962\")\n",
    "# # Transform into a DataArray\n",
    "# gridcoords = xr.DataArray(\n",
    "#     data=gridcoords.values,\n",
    "#     dims=(\"grid_index\", \"theta_phi\"),\n",
    "#     coords={\n",
    "#         \"grid_index\": gridcoords.index,\n",
    "#         \"theta_phi\": [\"theta\", \"phi\"]\n",
    "#     }\n",
    "# )\n",
    "gridcoords[\"Latitude\"] = 90 - gridcoords[\"theta\"]\n",
    "gridcoords[\"Longitude\"] = np.vectorize(lambda x: x if x <= 180 else x - 360)(gridcoords[\"phi\"])\n",
    "gridcoords[\"QDLat\"] = 90 - gridcoords[\"theta\"]\n",
    "gridcoords[\"MLT\"] = gridcoords[\"phi\"]/15\n",
    "gridcoords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5296a-3b31-49b3-b879-91f844134747",
   "metadata": {},
   "source": [
    "## Reduce down to what we might actually work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33038d-ea2d-4010-a9bb-10ec973bdd1a",
   "metadata": {},
   "source": [
    "`ds` points to the original dataset. In some cases we will just use `_ds` (defined below) which points to a subset of the data.\n",
    "\n",
    "We will only consider the variable `B_NEC_res_CHAOS-full` which is the residual to the full CHAOS model (which parameterises the core, crustal, and magnetospheric fields), i.e. approximately the magnetic disturbance created by the ionosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728236b-05bd-404e-8195-82f10e971222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select out some interesting parameters to work with\n",
    "_ds = ds[\n",
    "    [\n",
    "        \"B_NEC_res_CHAOS-full\",\n",
    "        \"Latitude\", \"Longitude\", \"QDLat\", \"QDLon\", \"MLT\",\n",
    "        # \"SunZenithAngle\", \"OrbitNumber\",\n",
    "        \"gridpoint_geo\", \"gridpoint_qdmlt\",\n",
    "        \"IMF_BY\", \"IMF_BZ\", \"IMF_Em\", \"IMF_V\",\n",
    "        # \"Kp\", \"RC\", \"dRC\"\n",
    "    ]\n",
    "]\n",
    "# Downsampled by 1/60 (i.e. 10-minute sampling) to make it easier to work on prototyping\n",
    "_ds = _ds.isel(Timestamp=slice(0, -1, 60))\n",
    "_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2cd18-5260-4a10-ba3a-716a3d973ca1",
   "metadata": {},
   "source": [
    "Since we loaded the zarr using Dask, the dataset is not actually in memory. That will be useful later for scaling to the full dataset, but for now let's load it into memory (so we can forget about Dask for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75071480-b95c-4cda-b4b4-688d5a668078",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a1321-29e7-4fb6-8abe-c622b05edb99",
   "metadata": {},
   "source": [
    "Some notes on those parameters:\n",
    "- The `IMF_..` variables are created from OMNI data which has already been time-shifted to Earth's bow shock, but also time-averaged over 20 minutes to give a smoothed input of energy to the magnetosphere and account for typical lag times from the input at the magnetopause to the response in the ionosphere. This should be pretty much the same input as that used in the AMPS model. For a better model we should use the full original OMNI data as input (both to give the full time-history to drive the model better, but also open up the opportunity for the model to account for varying lag times - we expect a more prompt response on the dayside, \"direct driving\", and a much more variable lagged response on the nightside, \"magnetospheric unloading\").\n",
    "- `IMF_Em` is merging electric field (a type of coupling function, composed from `BY`, `BZ` & `V`)\n",
    "- `gridpoint_geo` and `gridpoint_qdmlt` are indexes into positions in spherical grids of 40962 points. Use the `gridcoords` dataframe to identify coordinates of those points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82799970-1592-4703-a1e7-ba053898d21e",
   "metadata": {},
   "source": [
    "## Some inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d3aec-060a-4d1d-94a1-ea431d84c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds.plot.scatter(\n",
    "    x=\"Longitude\", y=\"Latitude\", hue=\"B_NEC_res_CHAOS-full\",\n",
    "    s=0.1, cmap=nio_colormap(), col=\"NEC\", robust=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774042f8-e604-49fe-8631-e32e8ecb49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds.plot.scatter(\n",
    "    x=\"MLT\", y=\"QDLat\", hue=\"B_NEC_res_CHAOS-full\",\n",
    "    s=0.1, cmap=nio_colormap(), col=\"NEC\", robust=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647af4a2-07f3-417d-9477-f8c96fd49571",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds.plot.scatter(\n",
    "    x=\"Latitude\", y=\"B_NEC_res_CHAOS-full\", hue=\"IMF_Em\",\n",
    "    s=0.1, cmap=\"plasma\", col=\"NEC\", robust=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ca00-d7f1-424b-a739-6c0aa7c83861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_gridpoint(_ds=_ds, index=0, x=\"IMF_Em\"):\n",
    "    \"\"\"Quick scatterplot of data in a given bin\"\"\"\n",
    "    # Select data from given gridpoint\n",
    "    __ds = _ds.where(ds[\"gridpoint_qdmlt\"]==index, drop=True)\n",
    "    # Identify coordinates of that bin\n",
    "    qdlat, mlt = gridcoords.iloc[index][[\"QDLat\", \"MLT\"]]\n",
    "    # Construct figure\n",
    "    facetgrid = __ds.plot.scatter(x=x, y=\"B_NEC_res_CHAOS-full\", col=\"NEC\")\n",
    "    # Add coordinates for the displayed gridpoint\n",
    "    facetgrid.fig.suptitle(\n",
    "        f\"Grid point: QDLat={np.round(qdlat, 1)}, MLT={np.round(mlt, 1)}\",\n",
    "        verticalalignment=\"bottom\"\n",
    "    )\n",
    "    return facetgrid\n",
    "\n",
    "\n",
    "# Plot a few different bins. NB: We use the full dataset this time\n",
    "for index in range(0, 5):\n",
    "    inspect_gridpoint(_ds=ds, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cdfbb-9f3c-48e1-90fb-f8c885c6cb34",
   "metadata": {},
   "source": [
    "## A simple model based on linear regressions within each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cac801-9703-4152-ac1b-c4fb7d98f907",
   "metadata": {},
   "source": [
    "Within each bin, we perform a linear regression of `B_NEC_res_CHAOS-full` against `IMF_Em`.\n",
    "\n",
    "*Using the full dataset `ds` this time - but we chop it in half - using the first half for training and the second half for testing*\n",
    "\n",
    "- We ignore the radial variation of data within each bin. This is okay because data are collected at a similar altitude since we only consider one satellite over five years. We will need to re-think this when extending to longer duration and multiple satellites.\n",
    "- Use only the vertical component, just to keep things simpler. The vertical component is the more stable one so easier to predict (maybe?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa120b-d7e5-4250-af5c-413be0098c02",
   "metadata": {},
   "source": [
    "Divide the dataset in two: (use the first half to train the model, and the second half to test it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5a1b-6135-467c-8666-43089a90db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoint = int(len(ds[\"Timestamp\"])/2)\n",
    "_ds_train = ds.isel(Timestamp=slice(0, midpoint, 1))  # used to build the model\n",
    "_ds_test = ds.isel(Timestamp=slice(midpoint, -1, 1))  # used to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313da85-5f00-448f-a5c3-bf8d0908aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_train[\"Timestamp\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce9070-2432-47e5-83cd-306955086c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_test[\"Timestamp\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09020c99-75c5-4050-bd59-00e34ecb00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747eadc-57bc-4339-91a0-73c8b176e548",
   "metadata": {},
   "source": [
    "First attempt using groupby-apply workflow didn't work out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b609b7-5ec7-40fb-bda5-f215f2593c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regress_xarray(ds, x=\"IMF_Em\", y=\"B_NEC_res_CHAOS-full\"):\n",
    "#     \"\"\"Regress variables within a dataset against each other\n",
    "\n",
    "#     Returns a DataArray so that it can be used in a groupby-apply workflow...\n",
    "#     Construction of this must be really slow - need to investigate how to do this properly\n",
    "#     \"\"\"\n",
    "#     regression = linregress(ds[x], ds[y])\n",
    "#     # return [regression.slope, regression.intercept]\n",
    "#     return xr.DataArray(\n",
    "#         data=[regression.slope, regression.intercept],\n",
    "#         dims=[\"slope_intercept\"], coords={\"slope_intercept\": [\"slope\", \"intercept\"]}\n",
    "#     )\n",
    "\n",
    "# %%time\n",
    "# regression_results = _ds_train.sel(NEC=\"C\").groupby(\"gridpoint_qdmlt\").apply(regress_xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386bdcd5-0063-4c04-944d-2cfb7f0dbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ds=_ds_train):\n",
    "    \"\"\"Returns a DataArray containing slopes & intercepts of each linear regression\"\"\"\n",
    "    x = \"IMF_Em\"\n",
    "    y = \"B_NEC_res_CHAOS-full\"\n",
    "    N = len(_ds_train[\"Timestamp\"])\n",
    "    chunksize = 100000\n",
    "    ds = ds.chunk(chunksize)\n",
    "    # Arrange the x-y data to be regressed\n",
    "    # Read the data from the input Dataset (ds) and put in a simpler DataArray\n",
    "    regression = xr.DataArray(\n",
    "        data=np.empty((N, 2)),\n",
    "        dims=[\"Timestamp\", \"dim_1\"],\n",
    "        coords={\n",
    "            \"gridpoint_qdmlt\": ds[\"gridpoint_qdmlt\"]\n",
    "        }\n",
    "    ).chunk(chunksize)\n",
    "    regression.data[:, 0] = ds[x].data\n",
    "    regression.data[:, 1] = ds[y].sel(NEC=\"C\").data\n",
    "    # Load it into memory - not doing so makes it very slow\n",
    "    regression.load()\n",
    "    # Remove entries with NaNs so that lingress works\n",
    "    regression = regression.where(~np.any(np.isnan(regression), axis=1), drop=True)\n",
    "\n",
    "    def _regress(da):\n",
    "        result = linregress(da.data[:, 0], da.data[:, 1])\n",
    "        return [result.slope, result.intercept]\n",
    "\n",
    "    regression_results = np.empty((40962, 2))\n",
    "    regression_results[:] = np.nan\n",
    "    # Split dataset into the bins and apply the regression within each bin.\n",
    "    for i, __da in tqdm(regression.groupby(\"gridpoint_qdmlt\")):\n",
    "        regression_results[i] = _regress(__da)\n",
    "\n",
    "    regression_results = xr.DataArray(\n",
    "        data=regression_results,\n",
    "        dims=[\"gridpoint_qdmlt\", \"slope_intercept\"],\n",
    "        coords={\n",
    "            \"gridpoint_qdmlt\": range(40962),\n",
    "            \"slope_intercept\": [\"slope\", \"intercept\"]\n",
    "        }\n",
    "    )\n",
    "    return regression_results\n",
    "\n",
    "regression_results = build_model(ds=_ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8ed63-1f68-4978-a728-04ac134dca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba9a85-8903-49a0-9183-0e034e22b46d",
   "metadata": {},
   "source": [
    "### Apply the regression results to make the predictions over the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc1eb5-299e-4b7b-9cf5-7fdfc5cd3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(ds, regression_results):\n",
    "    \"\"\"Returns a DataArray of predictions for B_NEC_res based on IMF_Em\"\"\"\n",
    "    # Create a DataArray to hold the predictions\n",
    "    prediction = xr.DataArray(\n",
    "        data=np.empty((len(ds[\"Timestamp\"]), 2)),\n",
    "        coords={\n",
    "            \"gridpoint_qdmlt\": ds[\"gridpoint_qdmlt\"].data,\n",
    "            \"slope_intercept\": [\"slope\", \"intercept\"],\n",
    "        },\n",
    "        dims=(\"gridpoint_qdmlt\", \"slope_intercept\")\n",
    "    )\n",
    "    # Reorganise the regression results to follow the order of the gridpoints in the dataset\n",
    "    prediction.data = regression_results.reindex_like(prediction).data\n",
    "    # Apply the regression results, inplace in the prediction DataArray\n",
    "    m = prediction.sel({\"slope_intercept\": \"slope\"}).data\n",
    "    c = prediction.sel({\"slope_intercept\": \"intercept\"}).data\n",
    "    prediction.data[:, 0] = m * ds[\"IMF_Em\"].data + c\n",
    "    # Drop the unneeded dimension\n",
    "    prediction = prediction.isel({\"slope_intercept\": 0}).drop(\"slope_intercept\")\n",
    "    # Set Timestamp as the coordinate\n",
    "    prediction = prediction.rename({\"gridpoint_qdmlt\": \"Timestamp\"})\n",
    "    prediction[\"Timestamp\"] = ds[\"Timestamp\"]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e84d6-03cf-4e04-b632-e3c534a6b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = make_predictions(_ds_test, regression_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13acad4b-ff63-4118-bc53-fccdf825b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove some of the unreasonably large predictions\n",
    "# prediction = prediction.where(~(np.fabs(prediction) > 1000))\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca43c72-57eb-4abc-98b7-a270a4e49d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a4457-078a-4113-a5b4-c169cd1c450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(_ds_test[\"B_NEC_res_CHAOS-full\"].sel(NEC=\"C\").data, prediction.data, s=0.1)\n",
    "ax.set_xlabel(\"B_C_res (measured)\")\n",
    "ax.set_ylabel(\"B_C_res (predicted from merging electric field)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905397b-06b9-40d5-b638-50c5ef3964b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_test[\"prediction\"] = prediction\n",
    "_ds_test[\"prediction_residual\"] = prediction - \\\n",
    "    _ds_test[\"B_NEC_res_CHAOS-full\"].sel(NEC=\"C\")\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "_ds_test.isel(Timestamp=slice(0, -1, 60)).sel(NEC=\"C\").plot.scatter(\n",
    "    x=\"MLT\", y=\"QDLat\", hue=\"B_NEC_res_CHAOS-full\",\n",
    "    s=0.1, cmap=nio_colormap(), robust=True, ax=axes[0]\n",
    ")\n",
    "_ds_test.isel(Timestamp=slice(0, -1, 60)).plot.scatter(\n",
    "    x=\"MLT\", y=\"QDLat\", hue=\"prediction\",\n",
    "    s=0.1, cmap=nio_colormap(), robust=True, ax=axes[1]\n",
    ")\n",
    "_ds_test.isel(Timestamp=slice(0, -1, 60)).plot.scatter(\n",
    "    x=\"MLT\", y=\"QDLat\", hue=\"prediction_residual\",\n",
    "    s=0.1, cmap=nio_colormap(), robust=True, ax=axes[2]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geomagcubes]",
   "language": "python",
   "name": "conda-env-geomagcubes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
