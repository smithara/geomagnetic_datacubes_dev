{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bca716f-20c7-4052-b5ea-fbd8c467ec0b",
   "metadata": {},
   "source": [
    "# Prepare access to datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e8128-c90d-4f37-8577-fff471a5f62b",
   "metadata": {},
   "source": [
    "This notebook handles download of a pre-made datacube. It is required to run this first for the other notebooks to work. Eventually this step should be replaced by directly referencing a datacube in object storage.\n",
    "\n",
    "If you run the code below without modification it will generate two copies of the data cube, at:\n",
    "```\n",
    "geomagnetic_datacubes_dev/data/interim/SwA_20140501-20190501_proc1.nc\n",
    "geomagnetic_datacubes_dev/notebooks/datacube_test.zarr\n",
    "```\n",
    "(about 5GB x2)\n",
    "\n",
    "Note that this datacube has been prepared at only 10-second sampling (compared to the 1Hz / 1-second original data, or the 50Hz high resolution data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2b976-d5f6-40ee-b7c3-2d7febf38f47",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da49aa-be18-4f7b-8156-bfdda038eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pooch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "import zarr\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from chaosmagpy.plot_utils import nio_colormap\n",
    "\n",
    "try:\n",
    "    from src.data.proc_env import INT_FILE_PATHS\n",
    "    from src.env import REFRAD, TMPDIR\n",
    "except ImportError:\n",
    "    print(\"Failed to import src...\")\n",
    "    TMPDIR = os.getcwd()\n",
    "    INT_FILE_PATHS = {\"A\": os.path.join(TMPDIR, \"SwA_20140501-20190501_proc1.nc\")}\n",
    "    REFRAD = 6371200\n",
    "    print(\"Using instead for cube download and scratch space:\", TMPDIR)\n",
    "if not os.path.exists(TMPDIR):\n",
    "    print(\"Can't find scratch space:\", TMPDIR)\n",
    "    TMPDIR = os.getcwd()\n",
    "    print(\"Using instead:\", TMPDIR)\n",
    "\n",
    "xr.set_options(\n",
    "    display_expand_attrs=False,\n",
    "    display_expand_data_vars=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db277c0f-62c8-4a2c-891e-8512e5abc3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Using temporary working directory:\",\n",
    "    TMPDIR,\n",
    "    \"Is this a good location for data I/O? Configure this path in the file: geomagnetic_datacubes_dev/config.ini\",\n",
    "    \"(or manually enter new paths above if not using the geomagcubes environment)\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dc6ff-7956-49ba-bac5-b41697cae3a8",
   "metadata": {},
   "source": [
    "## Load/prepare datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e85585-3039-4f58-8364-984c44eae156",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download pre-made datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fbee4-5ec2-4f4b-9d15-7023c6a59ee7",
   "metadata": {},
   "source": [
    "This part to be refactored into the datacube generation pipeline, when a permanent link is made available.\n",
    "\n",
    "Download the file if we don't already have it available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f9fc1-4d43-45d7-9ff9-c9e229172d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location at which the data will be located\n",
    "fpath = INT_FILE_PATHS[\"A\"]\n",
    "path, fname = os.path.split(fpath)\n",
    "path, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f5ea6-a34d-4752-8d4d-fcb097c2a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete it if you want to redownload it\n",
    "# os.remove(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d68236-3859-4809-b000-c6b511ab76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(fpath):\n",
    "    # Skip the download if we already have the file\n",
    "    print(\"Already found file:\", fpath, sep=\"\\n\")\n",
    "    pass\n",
    "else:\n",
    "    pooch.retrieve(\n",
    "        url=\"https://nc.smithara.net/index.php/s/H5R923DsbtirfJy/download\",\n",
    "        known_hash=\"1b7a8cbc0cb1657f8d4444ae7f6bbab91841318e1a172fa1f8a487b9d9492912\",\n",
    "        path=path, fname=fname,\n",
    "        progressbar=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f5362-6dfd-4978-948b-f1b9a81b8d6e",
   "metadata": {},
   "source": [
    "### Make a copy of the input datacube as a Zarr store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38959010-0acb-4475-be87-b57a0a68db47",
   "metadata": {},
   "source": [
    "**Includes temporary fixes to the datacube**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb6934-1124-485d-9260-e0225a770f3c",
   "metadata": {},
   "source": [
    "We want to make sure we don't accidentally modify the input dataset, so let's work on a copy. There are also some opportunities with xarray and dask and the zarr format to increase performance by dividing into chunks / rearranging the data in different ways - the input data format is not necessarily what we want to use for computation. So here we convert the data to the [Zarr](https://zarr.readthedocs.io) format\n",
    "\n",
    "(could work with the .nc file just the same; not sure yet what the advantages of zarr are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b135e9f-0fe1-4f07-9241-51d50f87b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = INT_FILE_PATHS[\"A\"]\n",
    "zarr_store = os.path.join(TMPDIR, \"datacube_test.zarr\")\n",
    "print(\"Input file:\", file_in, \"Copying to:\", zarr_store, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ef877-0d86-4685-ab10-37e9f26556da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datacube(ds):\n",
    "    ds.attrs.pop(\"Sources\")\n",
    "    # Generate residuals to use\n",
    "    ds[\"B_NEC_res_CHAOS-full\"] = (\n",
    "        ds[\"B_NEC\"]\n",
    "        - ds[\"B_NEC_CHAOS-MCO\"]\n",
    "        - ds[\"B_NEC_CHAOS-MMA\"]\n",
    "        - ds[\"B_NEC_CHAOS-Static_n16plus\"]\n",
    "    )\n",
    "    # Remove unphysical outliers (TODO: remove them from the datacube!)\n",
    "    outliers = np.fabs((ds[\"B_NEC_res_CHAOS-full\"]**2).sum(axis=1)) > 2000**2\n",
    "    idx_to_scrub = np.argwhere(outliers.data)[:, 0]\n",
    "    # Replace the bad data with nan\n",
    "    ds[\"B_NEC\"].data[idx_to_scrub, :] = np.nan\n",
    "    ds[\"B_NEC_res_CHAOS-full\"].data[idx_to_scrub, :] = np.nan\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169e6b5-dbe6-4078-a0f8-1ba76691869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(zarr_store):\n",
    "    print(\"Already found zarr:\", zarr_store)\n",
    "else:\n",
    "    pbar = ProgressBar()\n",
    "    with pbar:\n",
    "        with xr.open_dataset(file_in, chunks=100000) as ds:\n",
    "            print(\"Cleaning and storing as zarr:\", zarr_store)\n",
    "            ds = clean_datacube(ds)\n",
    "            ds.to_zarr(zarr_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1a383-bf59-4fb5-8809-2f636055ab95",
   "metadata": {},
   "source": [
    "NB: The dataset has been created in chunks of size 100,000 (Each file within the zarr contains this number of measurements). This won't be optimal, but will require some experimentation to find better chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dc376-1408-4264-9aef-4454b7260ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To delete the zarr:\n",
    "\n",
    "# from shutil import rmtree\n",
    "\n",
    "# rmtree(zarr_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geomagcubes2]",
   "language": "python",
   "name": "conda-env-geomagcubes2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
